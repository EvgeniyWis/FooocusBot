import re
import traceback
from collections import defaultdict

from aiogram import types
from aiogram.filters import StateFilter
from aiogram.fsm.context import FSMContext
from helpers.handlers.startGeneration.regenerateImage import get_normal_model
from keyboards.startGeneration.keyboards import done_typing_keyboard
from pydantic import ValidationError
from utils.handlers.messages import safe_edit_message

from bot.app.config.constants import MULTI_IMAGE_NUMBER
from bot.helpers import text
from bot.helpers.generateImages.dataArray import (
    get_all_model_indexes,
    get_group_model_indexes,
    get_model_index_by_model_name,
    get_model_name_by_index,
    getAllDataArrays,
    getDataByModelName,
)
from bot.helpers.generateImages.dataArray.check_model_index_is_exist import (
    check_model_index_is_exist,
)
from bot.helpers.generateImages.generateImageBlock import generateImageBlock
from bot.helpers.handlers.messages import deleteMessageFromState
from bot.helpers.handlers.startGeneration import (
    generateImagesInHandler,
    process_image,
    regenerateImage,
)
from bot.app.instance import bot, start_generation_router
from bot.keyboards import (
    randomizer_keyboards,
    start_generation_keyboards,
)
from bot.app.core.logging import logger
from bot.states.StartGenerationState import (
    MultiPromptInputState,
    StartGenerationState,
)
from bot.utils.handlers import (
    appendDataToStateArray,
)
from bot.utils.handlers.messages import (
    LONG_PROMPT_PROCESSING_SPINNER_TEXT,
    editMessageOrAnswer,
)
from bot.utils.handlers.messages.rate_limiter_for_send_message import (
    safe_send_message,
)

PROMPT_BY_INDEX_PATTERN = re.compile(
    r"(?s)(\d+)\s*[:\-‚Äì‚Äî]\s*(.*?)(?=(?:\n\d+\s*[:\-‚Äì‚Äî])|\Z)",
)

all_model_indexes = get_all_model_indexes()


# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π
async def choose_generations_type(
    call: types.CallbackQuery,
    state: FSMContext,
):
    generations_type = call.data.split("|")[1]
    await state.update_data(generations_type=generations_type)

    await editMessageOrAnswer(
        call,
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:\n\n"
        f"üñº –ú—É–ª—å—Ç–∏–≤—ã–±–æ—Ä - –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –ø—Ä–∏—Å—ã–ª–∞–µ—Ç—Å—è {MULTI_IMAGE_NUMBER} –Ω–∞ –≤—ã–±–æ—Ä\n"
        "‚úÖ –û–¥–∏–Ω–æ—á–Ω—ã–π - –º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –ø—Ä–∏—Å—ã–ª–∞–µ—Ç—Å—è 4 –Ω–∞ –≤—ã–±–æ—Ä",
        reply_markup=start_generation_keyboards.generationModeKeyboard(),
    )


# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
async def choose_generation_mode(call: types.CallbackQuery, state: FSMContext):
    mode = call.data.split("|")[1]
    if mode == "multi_select":
        await state.update_data(multi_select_mode=True)
    else:
        await state.update_data(multi_select_mode=False)
    await editMessageOrAnswer(
        call,
        text.GET_GENERATIONS_SUCCESS_TEXT,
        reply_markup=start_generation_keyboards.selectGroupKeyboard(),
    )


# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ –≥—Ä—É–ø–ø—ã
async def choose_group(call: types.CallbackQuery, state: FSMContext):
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç–µ–π—Ç–∞
    state_data = await state.get_data()

    # –û—á–∏—â–∞–µ–º —Å—Ç–µ–π—Ç
    await state.clear()

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–µ–π—Ç–µ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
    initial_state = {
        "multi_select_mode": state_data.get("multi_select_mode", False),
        "generations_type": state_data.get("generations_type", ""),
    }
    await state.update_data(**initial_state)

    # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –º–æ–¥–µ–ª—å, —Ç–æ –ø—Ä–æ—Å–∏–º –≤–≤–µ—Å—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    if call.data == "select_group|specific_model":
        await safe_edit_message(
            call.message,
            "üñº –í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:",
            reply_markup=start_generation_keyboards.select_type_specific_generation(),
        )
        await state.update_data(specific_model=True)

        return

    # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–∞ –¥—Ä—É–≥–∞—è –≥—Ä—É–ø–ø–∞, —Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
    group_number = call.data.split("|")[1]
    await state.update_data(group_number=group_number)
    await state.update_data(specific_model=False)

    await editMessageOrAnswer(
        call,
        text.CHOOSE_WRITE_PROMPT_TYPE_SUCCESS_TEXT,
        reply_markup=start_generation_keyboards.writePromptTypeKeyboard(),
    )


# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞ –Ω–∞–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞
async def choose_writePrompt_type(
    call: types.CallbackQuery,
    state: FSMContext,
):
    # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø: one –∏–ª–∏ multi
    prompt_type = call.data.split("|")[1]
    await state.update_data(writePrompt_type=prompt_type)

    if prompt_type == "one":
        # –û–¥–∏–Ω –ø—Ä–æ–º–ø—Ç –Ω–∞ –≤—Å–µ –º–æ–¥–µ–ª–∏
        await editMessageOrAnswer(
            call,
            text.GET_ONE_PROMPT_GENERATION_SUCCESS_TEXT,
            reply_markup=start_generation_keyboards.onePromptGenerationChooseTypeKeyboard(),
        )
        return

    await start_write_prompts_for_models_multiline_input(call, state)


async def start_write_prompts_for_models_multiline_input(
    callback: types.CallbackQuery,
    state: FSMContext,
):
    state_data = await state.get_data()
    group_number = state_data.get("group_number", 1)

    # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã –º–æ–¥–µ–ª–µ–π
    if group_number == "all":
        # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–æ all ‚Äî –±–µ—Ä—ë–º –≤—Å–µ –º–æ–¥–µ–ª–∏
        all_data_arrays = getAllDataArrays()
        start_index = 1
        end_index = sum(len(group) for group in all_data_arrays)
    else:
        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –≥—Ä—É–ø–ø—ã
        all_data_arrays = getAllDataArrays()
        group_index = int(group_number) - 1

        # –°—á–∏—Ç–∞–µ–º —Å–º–µ—â–µ–Ω–∏–µ –∫–∞–∫ —Å—É–º–º—É –¥–ª–∏–Ω –≤—Å–µ—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–µ—Ç–æ–≤
        offset = sum(len(arr) for arr in all_data_arrays[:group_index])

        # –î–ª–∏–Ω–∞ —Ç–µ–∫—É—â–µ–≥–æ —Å–µ—Ç–∞
        group_length = len(all_data_arrays[group_index])

        start_index = offset + 1
        end_index = offset + group_length

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ —Å—Ç–µ–π—Ç
    await state.update_data(
        valid_model_indexes_range=(start_index, end_index),
    )
    await state.update_data(
        prompt_chunks=[],
    )

    group_model_indexes = get_group_model_indexes(group_number)

    await safe_send_message(
        text.WRITE_PROMPTS_FOR_MODELS_TEXT.format(group_model_indexes),
        message=callback.message,
        reply_markup=done_typing_keyboard(),
    )
    await state.set_state(
        MultiPromptInputState.collecting_model_prompts_for_groups,
    )


# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ "–∏–Ω–¥–µ–∫—Å: –ø—Ä–æ–º–ø—Ç" –¥–ª—è —Ç–µ–∫—É—â–µ–π –≥—Ä—É–ø–ø—ã
async def write_prompts_for_models(message: types.Message, state: FSMContext):
    text_input = message.text.strip()
    matches = PROMPT_BY_INDEX_PATTERN.findall(text_input)

    if not matches:
        await safe_send_message(
            text.EMPTY_MATCHES_WRITE_PROMPTS_TEXT,
            message,
        )
        return

    state_data = await state.get_data()
    valid_range = state_data.get("valid_model_indexes_range", (1, 100))
    start_index, end_index = valid_range
    user_id = message.from_user.id
    expected_count = end_index - start_index + 1
    group_number = state_data.get("group_number", "1")

    model_prompts = {}
    prompt_counter = defaultdict(int)
    unique_model_indexes: set[int] = set()

    for index_str, prompt in matches:
        index_base = int(index_str.split("+")[0])

        if not check_model_index_is_exist(index_base):
            await safe_send_message(
                text.MODEL_NOT_FOUND_TEXT.format(
                    index_base,
                    all_model_indexes,
                ),
                message,
            )
            await state.update_data(prompt_chunks=[])
            return

        unique_model_indexes.add(index_base)

        count = prompt_counter[str(index_base)]
        full_key = str(index_base) if count == 0 else f"{index_base}+{count}"
        model_prompts[full_key] = prompt.strip()
        prompt_counter[str(index_base)] += 1

    if len(unique_model_indexes) != expected_count:
        await safe_send_message(
            f"‚ö†Ô∏è –ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å <b>—Ä–æ–≤–Ω–æ {expected_count}</b> –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏ (–∞ –Ω–µ {len(unique_model_indexes)}).",
            message,
        )
        return

    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤
    state_data = await state.get_data()
    current_prompts = state_data.get("model_prompts_for_generation", [])
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    for key, prompt in model_prompts.items():
        new_prompt_data = {
            "model_name": f"{get_model_name_by_index(key)}_{key}", 
            "prompt": prompt
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π –ø—Ä–æ–º–ø—Ç
        updated = False
        for idx, item in enumerate(current_prompts):
            if item.get("model_name") == new_prompt_data["model_name"]:
                current_prompts[idx] = new_prompt_data
                updated = True
                break
        
        if not updated:
            current_prompts.append(new_prompt_data)
    
    await state.update_data(model_prompts_for_generation=current_prompts)

    await safe_send_message(
        "‚úÖ –ü—Ä–æ–º–ø—Ç—ã –ø–æ–ª—É—á–µ–Ω—ã. –ù–∞—á–∏–Ω–∞—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...",
        message,
    )

    try:
        await generateImagesInHandler(
            prompt=model_prompts,
            message=message,
            state=state,
            user_id=user_id,
            group_number=group_number,
            with_randomizer=False,
        )
    except Exception:
        await safe_send_message("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", message)
        return


# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –æ–¥–Ω–∏–º –ø—Ä–æ–º–ø—Ç–æ–º
async def chooseOnePromptGenerationType(
    call: types.CallbackQuery,
    state: FSMContext,
):
    one_prompt_generation_type = call.data.split("|")[1]

    if one_prompt_generation_type == "static":
        await editMessageOrAnswer(
            call,
            text.GET_STATIC_PROMPT_TYPE_SUCCESS_TEXT,
        )
        await state.set_state(StartGenerationState.write_prompt_for_images)

    elif one_prompt_generation_type == "random":
        # –û—á–∏—â–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ —Ä–∞–Ω–¥–æ–º–∞–π–∑–µ—Ä–µ
        await state.update_data(variable_names_for_randomizer=[])
        await state.update_data(variable_name_values=[])
        await editMessageOrAnswer(
            call,
            text.GET_RANDOM_PROMPT_TYPE_SUCCESS_TEXT,
            reply_markup=randomizer_keyboards.randomizerKeyboard([]),
        )


# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–≤–æ–¥–∞ –ø—Ä–æ–º–ø—Ç–∞
async def write_prompt(message: types.Message, state: FSMContext):
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    prompt = message.text
    user_id = message.from_user.id
    state_data = await state.get_data()
    await state.update_data(prompt_for_images=prompt)

    await state.set_state(None)

    # –ï—Å–ª–∏ –≤ —Å—Ç–µ–π—Ç–µ –µ—Å—Ç—å –Ω–æ–º–µ—Ä –≥—Ä—É–ø–ø—ã, —Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –ø–æ–ª—É—á–∞–µ–º –Ω–æ–º–µ—Ä –≥—Ä—É–ø–ø—ã –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏
    if "group_number" in state_data:
        group_number = state_data.get("group_number", 1)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        await generateImagesInHandler(
            prompt,
            message,
            state,
            user_id,
            group_number,
        )
    else:
        model_indexes = state_data.get("model_indexes_for_generation", [])
        logger.info(f"–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {model_indexes}")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        await generateImagesInHandler(
            prompt,
            message,
            state,
            user_id,
            "individual",
        )


async def get_model_name_with_job_id(
    state: FSMContext,
    job_id: str,
) -> str:
    state_data = await state.get_data()
    mapping: dict = state_data.get(
        "job_id_to_full_model_key",
        {},
    )
    logger.info(
        f"–ò—â–µ–º model_name –ø–æ job_id: job_id={job_id}",
    )
    logger.info(f"job_id_to_full_model_key: {mapping}")

    model_name: str | None = None
    for full_job_id, model_key in mapping.items():
        if full_job_id.startswith(job_id):
            model_name = model_key
            break

    return model_name


# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
async def select_image(call: types.CallbackQuery, state: FSMContext):
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –≤—ã–±–æ—Ä–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    await editMessageOrAnswer(
        call,
        text.SELECT_IMAGE_PROGRESS_TEXT,
    )

    full_model_key, group_number, image_index, job_id_prefix = (
        call.data.split("|")[1:]
    )

    # –ò–∑–≤–ª–µ–∫–∞–µ–º model_name –∏ model_key –∏–∑ –ø–æ–ª–Ω–æ–≥–æ –∫–ª—é—á–∞
    if "/" in full_model_key:
        model_name, model_key = full_model_key.rsplit("/", 1)
    else:
        model_name = full_model_key
        model_key = None

    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏
    data = await getDataByModelName(model_name)

    # –£–¥–∞–ª—è–µ–º –º–µ–¥–∏–∞–≥—Ä—É–ø–ø—É
    logger.info(f"–£–¥–∞–ª—è–µ–º –º–µ–¥–∏–∞–≥—Ä—É–ø–ø—É –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}")
    await deleteMessageFromState(
        state,
        "imageGeneration_mediagroup_messages_ids",
        model_name,
        call.message.chat.id,
        job_id=job_id_prefix,
    )
    try:
        if image_index == "regenerate":
            model_name_for_regenerate = (
                await get_model_name_with_job_id(
                    state,
                    job_id_prefix,
                )
            )

            if not model_name_for_regenerate:
                logger.warning(
                    f"[regenerate] –ù–µ –Ω–∞–π–¥–µ–Ω model_name_for_regenerate –¥–ª—è job_id_prefix={job_id_prefix}",
                )
                raise Exception(
                    "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.",
                )

            return await regenerateImage(
                model_name_for_regenerate,
                call,
                state,
            )
        elif image_index == "prompt_regen":
            model_name_for_regenerate = (
                await get_model_name_with_job_id(
                    state,
                    job_id_prefix,
                )
            )

            if not model_name_for_regenerate:
                logger.warning(
                    f"[regenerate_with_new_prompt] –ù–µ –Ω–∞–π–¥–µ–Ω model_name_for_regenerate –¥–ª—è job_id_prefix={job_id_prefix}",
                )
                raise Exception(
                    "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –Ω–æ–≤–æ–º—É –ø—Ä–æ–º–ø—Ç—É.",
                )

            await state.update_data(
                group_number_for_regenerate_image=group_number,
                model_name_for_regenerate_image=model_name_for_regenerate,
                job_id_for_regenerate=job_id_prefix,
            )

            await state.set_state(
                StartGenerationState.write_new_prompt_for_regenerate_image,
            )

            write_new_prompt_for_regenerate_message = (
                await editMessageOrAnswer(call, text.WRITE_NEW_PROMPT_TEXT)
            )
            await state.update_data(
                write_new_prompt_message_id=write_new_prompt_for_regenerate_message.message_id,
            )
            return

        image_index = int(image_index)

        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—â–µ–º –≤–æ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–∞—Å—Å–∏–≤–∞—Ö
        if data is None:
            all_data_arrays = getAllDataArrays()
            for arr in all_data_arrays:
                data = next(
                    (d for d in arr if d["model_name"] == model_name),
                    None,
                )
                if data is not None:
                    break

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ id –ø–∞–ø–∫–∏ –¥–ª—è –≤–∏–¥–µ–æ
        await state.update_data(model_name=model_name)

        if not call.message:
            return await editMessageOrAnswer(
                call,
                "–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ!",
            )

        logger.info("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
        await process_image(
            call,
            state,
            model_name,
            image_index,
            model_key=model_key,
        )
    except Exception as e:
        traceback.print_exc()
        model_name_index = get_model_index_by_model_name(model_name)

        await editMessageOrAnswer(
            call,
            text.GENERATE_IMAGE_ERROR_TEXT.format(
                model_name,
                model_name_index,
                e,
            ),
        )
        raise e


async def start_multi_prompt_input_mode(
    callback: types.CallbackQuery,
    state: FSMContext,
):
    await state.set_state(MultiPromptInputState.collecting_prompt_parts)
    await state.update_data(
        prompt_chunks=[],
    )

    await safe_edit_message(
        callback.message,
        text.WRITE_MULTI_PROMPTS_FOR_SPECIFIC_GENERATION,
        reply_markup=done_typing_keyboard(),
    )


async def handle_chunk_input(message: types.Message, state: FSMContext):
    data = await state.get_data()
    chunks = data.get("prompt_chunks", [])
    msg = message.text.strip()

    if not msg:
        await safe_send_message(
            text.EMPTY_PROMPT_TEXT,
            message,
        )
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç "–∏–Ω–¥–µ–∫—Å: –ø—Ä–æ–º–ø—Ç"
    matches = PROMPT_BY_INDEX_PATTERN.findall(msg)
    if matches:
        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω —Ñ–æ—Ä–º–∞—Ç "–∏–Ω–¥–µ–∫—Å: –ø—Ä–æ–º–ø—Ç", –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        for index_str, _ in matches:
            index_base = int(index_str.split("+")[0])

            if not check_model_index_is_exist(index_base):
                await safe_send_message(
                    text.MODEL_NOT_FOUND_TEXT.format(
                        index_base,
                        all_model_indexes,
                    ),
                    message,
                )
                return
    else:
        # –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
        await safe_send_message(
            text.WRONG_FORMAT_TEXT,
            message,
        )
        return

    chunks.append(msg)
    await state.update_data(
        prompt_chunks=chunks,
        last_user_id=message.from_user.id,
        last_chat_id=message.chat.id,
        last_message_id=message.message_id,
    )
    await safe_send_message(
        text.MESSAGE_IS_SUCCESFULLY_DONE,
        message,
        reply_markup=done_typing_keyboard(),
    )


async def finish_prompt_input(
    callback: types.CallbackQuery,
    state: FSMContext,
):
    data = await state.get_data()
    full_text = "\n".join(data.get("prompt_chunks", []))
    prompt_chunks = data.get("prompt_chunks", [])
    if not prompt_chunks:
        await safe_edit_message(
            callback.message,
            "‚ùóÔ∏è–í—ã –Ω–µ –≤–≤–µ–ª–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞.",
        )
        return

    user_id = data.get("last_user_id") or callback.from_user.id
    chat_id = data.get("last_chat_id") or callback.message.chat.id

    await safe_edit_message(
        callback.message,
        LONG_PROMPT_PROCESSING_SPINNER_TEXT,
    )
    try:
        fake_message = types.Message(
            message_id=callback.message.message_id,
            date=callback.message.date,
            chat=types.Chat(id=chat_id, type="private"),
            from_user=callback.from_user,
            text=full_text,
        )
    except ValidationError:
        logger.exception(
            f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ finish_prompt_input –¥–ª—è user_id={user_id}, chat_id={chat_id}",
        )
        await safe_edit_message(
            callback.message,
            "‚ùóÔ∏è–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–º–ø—Ç–∞.",
        )
        return

    current_state = await state.get_state()

    if (
        current_state
        == MultiPromptInputState.collecting_model_prompts_for_groups.state
    ):
        await write_prompts_for_models(
            message=fake_message,
            state=state,
        )
    else:
        await write_model_for_generation(
            message=fake_message,
            state=state,
            text_input=full_text,
        )


async def send_message_with_info_for_write_prompts_for_models(
    callback: types.CallbackQuery,
    state: FSMContext,
):
    await safe_edit_message(
        callback.message,
        text.WRITE_MODELS_NAME_TEXT,
    )
    await state.set_state(
        StartGenerationState.write_models_for_specific_generation,
    )


async def write_models_for_specific_generation(
    message: types.Message,
    state: FSMContext,
):
    message_text = message.text.strip()

    if not all(x.strip().isdigit() for x in message_text.split(",")):
        await safe_send_message(
            text=text.NOT_NUMBER_TEXT,
            message=message,
        )
        return

    raw_model_indexes = [x.strip() for x in message_text.split(",")]
    model_indexes = make_unique_model_keys(raw_model_indexes)

    for model_index in model_indexes:
        if not check_model_index_is_exist(int(model_index)):
            await safe_send_message(
                text=text.MODEL_NOT_FOUND_TEXT.format(
                    model_index,
                    all_model_indexes,
                ),
                message=message,
            )
            await state.update_data(prompt_chunks=[])
            return

    await state.update_data(model_indexes_for_generation=model_indexes)

    if len(model_indexes) == 1:
        await safe_send_message(
            text=text.GET_MODEL_INDEX_SUCCESS_TEXT,
            message=message,
        )

        await state.set_state(StartGenerationState.write_prompt_for_images)

    else:
        await safe_send_message(
            text=text.GET_MODELS_INDEXES_AND_WRITE_PROMPT_TYPE_SUCCESS_TEXT,
            message=message,
            reply_markup=start_generation_keyboards.onePromptGenerationChooseTypeKeyboard(),
        )


def make_unique_model_keys(model_indexes: list[str]) -> list[str]:
    counter = defaultdict(int)
    result = []
    for index in model_indexes:
        count = counter[index]
        result.append(index if count == 0 else f"{index}+{count}")
        counter[index] += 1
    return result


async def write_model_for_generation(
    message: types.Message,
    state: FSMContext,
    text_input: str | None = None,
):
    text_input = text_input or message.text.strip()
    matches = PROMPT_BY_INDEX_PATTERN.findall(text_input)

    if not matches:
        await safe_send_message(text=text.WRONG_FORMAT_TEXT, message=message)
        return

    prompt_counter = defaultdict(int)
    model_prompts = {}

    for index, prompt in matches:
        if not index.isdigit():
            continue
        if not check_model_index_is_exist(int(index)):
            await safe_send_message(
                text=text.MODEL_NOT_FOUND_TEXT.format(
                    index,
                    all_model_indexes,
                ),
                message=message,
            )
            await state.update_data(prompt_chunks=[])
            return

        count = prompt_counter[index]
        key = index if count == 0 else f"{index}+{count}"
        model_prompts[key] = prompt.strip()
        prompt_counter[index] += 1

    data_for_update = [
        {"model_name": f"{get_model_name_by_index(key)}_{key}", "prompt": prompt}
        for key, prompt in model_prompts.items()
    ]

    logger.info(f"–û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç—ã –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏: {data_for_update}")
    await state.update_data(model_prompts_for_generation=data_for_update)

    message_to_del = await safe_send_message(
        text="‚úÖ –ü—Ä–æ–º–ø—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º –ø–æ–ª—É—á–µ–Ω—ã, –Ω–∞—á–∏–Ω–∞—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...",
        message=message,
    )
    await state.update_data(message_to_del=message_to_del.message_id)
    await generateImagesInHandler(
        prompt=model_prompts,
        message=message,
        state=state,
        user_id=message.from_user.id,
        group_number="individual",
    )


# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–≤–æ–¥–∞ –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
async def write_new_prompt_for_regenerate_image(
    message: types.Message,
    state: FSMContext,
):
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    prompt = message.text
    if not prompt:
        await safe_send_message(
            text=text.EMPTY_PROMPT_TEXT,
            message=message,
        )
        return

    state_data = await state.get_data()
    user_id = message.from_user.id

    # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    await message.delete()

    # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –±–æ—Ç–∞
    write_new_prompt_message_id = state_data.get(
        "write_new_prompt_message_id",
        None,
    )
    if write_new_prompt_message_id:
        try:
            await bot.delete_message(user_id, write_new_prompt_message_id)
        except Exception as e:
            logger.error(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ"
                f"–Ω–æ–≤–æ–º—É –ø—Ä–æ–º–ø—Ç—É {write_new_prompt_message_id} - {e}",
            )

    model_name = state_data.get("model_name_for_regenerate_image", "")
    job_id = state_data.get("job_id_for_regenerate", "")

    if not job_id:
        logger.warning("–ù–µ—Ç job_id_for_regenerate –≤ state!")
        return

    data_for_update = {f"{model_name}": prompt}
    await appendDataToStateArray(
        state,
        "prompts_for_regenerated_models",
        data_for_update,
        unique_keys=("model_name"),
    )

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–π full_model_key –≤ –±–∞–∑–æ–≤–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏
    full_model_key = model_name
    if "/" in full_model_key:
        base_model_name, _ = full_model_key.rsplit("/", 1)
    else:
        base_model_name = full_model_key

    normal_model_name = await get_normal_model(base_model_name)

    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å –º–æ–¥–µ–ª–∏
    model_name_index = get_model_index_by_model_name(normal_model_name)

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    modified_prompt = prompt[:30] + "..." if len(prompt) > 30 else prompt
    regenerate_progress_message = await safe_send_message(
        text=text.REGENERATE_IMAGE_WITH_NEW_PROMPT_TEXT.format(
            normal_model_name,
            model_name_index,
            modified_prompt,
        ),
        message=message,
    )

    await state.set_state(None)

    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏
    data = await getDataByModelName(normal_model_name)

    if not data:
        logger.error(f"[write_new_prompt_for_regenerate_image] –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏: {normal_model_name} (–∏—Å—Ö–æ–¥–Ω—ã–π –∫–ª—é—á: {full_model_key})")
        await editMessageOrAnswer(
            message,
            text.REGENERATE_IMAGE_ERROR_TEXT.format(
                normal_model_name,
                model_name_index,
                "–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
            ),
        )
        await regenerate_progress_message.delete()
        return

    await generateImageBlock(
        data,
        regenerate_progress_message.message_id,
        state,
        user_id,
        prompt,
        False,
        chat_id=message.chat.id,
    )
    await regenerate_progress_message.delete()


# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
def hand_add():
    start_generation_router.callback_query.register(
        choose_generations_type,
        lambda call: call.data.startswith("generations_type"),
    )

    start_generation_router.callback_query.register(
        choose_generation_mode,
        lambda call: call.data.startswith("generation_mode"),
    )

    start_generation_router.callback_query.register(
        choose_group,
        lambda call: call.data.startswith("select_group"),
    )

    start_generation_router.callback_query.register(
        choose_writePrompt_type,
        lambda call: call.data.startswith("write_prompt_type"),
    )

    start_generation_router.callback_query.register(
        chooseOnePromptGenerationType,
        lambda call: call.data.startswith("one_prompt_generation_type"),
    )

    start_generation_router.message.register(
        write_prompt,
        StateFilter(StartGenerationState.write_prompt_for_images),
    )

    start_generation_router.callback_query.register(
        select_image,
        lambda call: call.data.startswith("select_image"),
    )

    start_generation_router.message.register(
        write_new_prompt_for_regenerate_image,
        StateFilter(
            StartGenerationState.write_new_prompt_for_regenerate_image,
        ),
    )
    start_generation_router.message.register(
        write_prompts_for_models,
        StateFilter(
            StartGenerationState.write_multi_prompts_for_models,
        ),
    )
    start_generation_router.callback_query.register(
        send_message_with_info_for_write_prompts_for_models,
        lambda call: call.data.startswith("specific_generation|one_prompt"),
    )

    start_generation_router.callback_query.register(
        start_multi_prompt_input_mode,
        lambda call: call.data.startswith("specific_generation|more_prompts"),
    )

    start_generation_router.message.register(
        handle_chunk_input,
        StateFilter(
            MultiPromptInputState.collecting_model_prompts_for_groups,
            MultiPromptInputState.collecting_prompt_parts,
        ),
    )

    start_generation_router.callback_query.register(
        finish_prompt_input,
        lambda call: call.data == "done_typing",
    )

    start_generation_router.message.register(
        write_models_for_specific_generation,
        StateFilter(StartGenerationState.write_models_for_specific_generation),
    )
